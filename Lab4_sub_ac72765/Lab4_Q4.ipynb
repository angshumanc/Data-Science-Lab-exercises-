{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "The process used here is the same as the one used in problem 3, except for CIFAR instead of MNIST. The results can be seen in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PGCqULayJSLz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "already_loaded = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jKaDvKOlJvhi"
   },
   "outputs": [],
   "source": [
    "if already_loaded:\n",
    "    openml_mnist = pickle.load(open('cifar.pkl', 'rb'))\n",
    "else:\n",
    "    openml_mnist = fetch_openml('CIFAR_10_small', as_frame=True)\n",
    "    pickle.dump(openml_mnist, open('cifar.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r1txkIf2LAF2"
   },
   "outputs": [],
   "source": [
    "X_full = openml_mnist.data.values\n",
    "Y_full = openml_mnist.target.values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_full, Y_full, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HRd9-i16LCCL"
   },
   "outputs": [],
   "source": [
    "#Grid of parameters to train the Random Forest over\n",
    "param_grid = {\n",
    "    'rfc__n_estimators': [10,50,100,200,300,500],\n",
    "    'rfc__max_depth': [10,20,30,40,50,None],\n",
    "    'rfc__min_samples_split': [2, 5, 10],\n",
    "    'rfc__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('rfc', RandomForestClassifier())])\n",
    "rforest = RandomizedSearchCV(pipe, param_grid, cv=3, n_iter=50, verbose=2, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "id": "HDUol-oaNi1y",
    "outputId": "be8d1c13-6f9a-4104-88cb-4a0729757d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=2)]: Done 150 out of 150 | elapsed: 97.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=50,\n",
       "                   n_jobs=2,\n",
       "                   param_distributions={'max_depth': [10, 20, 30, 40, 50, None],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [10, 50, 100, 200, 300,\n",
       "                                                         500]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforest.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hWvELm3DMYFo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 40}\n"
     ]
    }
   ],
   "source": [
    "print(rforest.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QecFAYiuHZFB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with best Random Forest: 0.457\n"
     ]
    }
   ],
   "source": [
    "rforest_preds = rforest.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(Y_test, rforest_preds)\n",
    "print('Accuracy with best Random Forest: {0:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AtoDaL8APtm1"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'xgb__n_estimators': np.arange(10,51,10),\n",
    "    'xgb__learning_rate': [0.3, 0.4, 0.5],\n",
    "    'xgb__max_depth' : [2, 5, 10, 15],\n",
    "    'xgb__colsample_bytree': [0.3,0.5],\n",
    "}\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('xgb', xgb.XGBClassifier())])\n",
    "xgb_model = RandomizedSearchCV(pipe, param_grid, cv=3, n_iter=50, verbose=2, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X86dwJbpMeAc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed: 104.2min\n",
      "[Parallel(n_jobs=2)]: Done 150 out of 150 | elapsed: 369.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           random_state=None, reg_alpha=None,\n",
       "                                           reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=50, n_jobs=2,\n",
       "                   param_distributions={'colsample_bytree': [0.3, 0.5],\n",
       "                                        'learning_rate': [0.3, 0.4, 0.5],\n",
       "                                        'max_depth': [2, 5, 10, 15],\n",
       "                                        'n_estimators': array([10, 20, 30, 40, 50])},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FHgEX6B2HZFQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.3, 'colsample_bytree': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print(xgb_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fff6_PkbLxL5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with best Gradient Boosting: 0.465\n"
     ]
    }
   ],
   "source": [
    "xgb_preds = xgb_model.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(Y_test, xgb_preds)\n",
    "print('Accuracy with best Gradient Boosting: {0:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see a small increase in performance of gradient boosting over random forests. In any case, the performance of both is similar, and lower than MNIST since here the dataset is much more complex."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab4_Q3_Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
